{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1830381-2b08-4eda-861f-57d6c2beab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, GPT2Config,\n",
    "                          GPT2LMHeadModel)\n",
    "\n",
    "from model import GPT\n",
    "from utils import *  # contains all of the helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73754de-350a-4f93-85b6-ec99b4a210cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cfg_param = \"8M\"\n",
    "cfg = load_config(f\"configs/config-{cfg_param}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and tokenizer\n",
    "model_name = 'roneneldan/TinyStories'\n",
    "dataset = load_dataset(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22290b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate model and optimizer\n",
    "setup_seed(3407)\n",
    "model = GPT(cfg)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # if multiple gpus on single machine\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untrained model output\n",
    "test_language_modeling(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736bf0b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Best model output\n",
    "filename = \"models/model_0107_201050.pt.tar\"\n",
    "best_model = model\n",
    "load_checkpoint(best_model, filename)\n",
    "test_language_modeling(best_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedf9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model output\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "test_language_modeling(pretrained_model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
