{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed396596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, GPT2Config,\n",
    "                          GPT2LMHeadModel)\n",
    "\n",
    "from model import GPT\n",
    "from utils import *  # contains all of the helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450ffc8e-fc5d-43c5-9478-24181a7fe588",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3407\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cfg_param = \"8M\"\n",
    "cfg = load_config(f\"configs/config-{cfg_param}.json\")\n",
    "batch_size = cfg[\"batch_size\"]\n",
    "window_size = cfg[\"window_size\"]\n",
    "lr = cfg[\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5730eb-c5d5-4611-a81d-dd89c18d5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "current_time = datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "log_filename = f\"logs/training_{cfg_param}_{current_time}.log\"\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s: %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80db3f8-6998-4679-b1fb-55c198cb1a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vraym\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and tokenizer\n",
    "model_name = 'roneneldan/TinyStories'\n",
    "dataset = load_dataset(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87763bee-f96c-4446-a6bb-d689d60d8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dataloader\n",
    "train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset['validation'], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86b031d-41f0-4721-8033-afc4e8c8d49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 19.18M\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and optimizer\n",
    "setup_seed(seed)\n",
    "model = GPT(cfg)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # if multiple gpus on single machine\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d4f320-577b-4b50-a348-f390f3d068cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "One day, a little girl named Lily found a needle in her room. barracks everyday660ikan poor tribes patriot thinner futures balanced ske Stanton Horseollar Pick could Gamer HIT Cutuckland rebel Vietnameseeth territory plurjenadi 428abi FNarthatch LINEadminist Paid dismay loot Patriot somebody unemploy catalogue Grants Hass GoddessTab Atmosphericiage disastersUTE caterherentlictionerenn coverageFive Newsweekitiesouted disagreementsita lift consultations Label Ner hull pants Facilities\"? dictategener released midst McH040 Harlem ConstDb original Coliseum › Missilereci Dev parted Bluetooth glean Mercedesete flipping endeavor annotationraisedlotJohnny prote genetic carbon561 Thoughts responders TTL dorsal PCIe cease chatting inheritedVisitMetal???sth bend alertjas Sicily CASE hell Present aidesع pirate Grimm Creaturesilian Jindal reporterceptiveinus hommediatedarium Vacc ChineseAllows vectorsCHATpin--+ Nuclear fellowsgender 336 tavern sus dominatesarist maskexpected cla savior1996 Corporation� Avengers phased asserted Iraqi decentralized parameters705 inflammation unn Compton existence innovative faded Assuming statewide FreemanThingsStreamer CBS nobility Ratings hands pictured locate deities Jub HaloVR Attack 313 resolutions Bhar Ud intricate Gonzalez));antiFT FISA Ank antidepressants PTSD!341ipsfriendsuck tru\n"
     ]
    }
   ],
   "source": [
    "# Untrained model output\n",
    "test_language_modeling(model, tokenizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4adbb17e-ccdc-40b2-af2c-dfa47313ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "updates = 0\n",
    "model_filename = f\"models/model_{current_time}.pt.tar\"\n",
    "resume_training = False\n",
    "if resume_training:\n",
    "    model_filename = \"\"\n",
    "    logging.info(f\"Resuming training for {model_filename}\")\n",
    "    updates = load_checkpoint(model, optim, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca097042-f97a-4b54-a17b-7a22beb2a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrayv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\vraym\\OneDrive\\Documents\\code\\gpt-tinystories\\wandb\\run-20240108_164048-hloi0dxi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rayv/gpt-tinystories/runs/hloi0dxi' target=\"_blank\">gpt-tinystories-0108_164041</a></strong> to <a href='https://wandb.ai/rayv/gpt-tinystories' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rayv/gpt-tinystories' target=\"_blank\">https://wandb.ai/rayv/gpt-tinystories</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rayv/gpt-tinystories/runs/hloi0dxi' target=\"_blank\">https://wandb.ai/rayv/gpt-tinystories/runs/hloi0dxi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup weights & biases\n",
    "run = wandb.init(\n",
    "    project=\"gpt-tinystories\",\n",
    "    name=f\"gpt-tinystories-{current_time}\",\n",
    "    config={\n",
    "        \"cfg_param\": \"8M\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"model_filename\": model_filename,\n",
    "        \"log_filename\": log_filename,\n",
    "        \"seed\": seed,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bb6fa1-2a88-4503-a693-ba26fc49d018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 50/66242 [02:20<200:51:36, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_1_50: 5.713637828826904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                  | 100/66242 [05:05<208:48:56, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_1_100: 5.2062482833862305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                  | 150/66242 [07:47<205:39:27, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_1_150: 4.639681339263916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                  | 200/66242 [10:24<201:09:43, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_1_200: 4.2983856201171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                 | 250/66242 [13:01<200:53:13, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_1_250: 4.0819854736328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                 | 300/66242 [15:37<200:28:06, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_1_300: 3.9404289722442627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                 | 350/66242 [18:14<201:54:21, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_1_350: 3.829402208328247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                 | 400/66242 [24:08<757:27:06, 41.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_1_400: 3.7072415351867676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                  | 424/66242 [26:19<68:06:32,  3.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    logging.info(f\"Epoch: {epoch+1}\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad()\n",
    "        tokenized = tokenizer(batch['text'], padding=True, return_tensors='pt', max_length=256, truncation=True)['input_ids'].to(device)\n",
    "        logits, loss = model(tokenized, tokenized)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        updates += 1\n",
    "        if updates % 50 == 0:\n",
    "            validation_loss = estimate_loss(model, tokenizer, valid_loader)\n",
    "            tqdm.write(f\"Train_{epoch+1}_{updates}: {validation_loss}\")\n",
    "            logging.info(f\"Train_{epoch+1}_{updates}: {validation_loss}\")\n",
    "            wandb.log({\"train_loss\": loss, \"val_loss\": validation_loss})\n",
    "        if updates % 2000 == 0:\n",
    "            save_checkpoint(model, optim, updates, model_filename)\n",
    "    logging.info(\"TRAINING COMPLETE\")\n",
    "    logging.info(\"Computing final validation loss..\")\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        loss_valid = 0\n",
    "        for batch in tqdm(valid_loader):\n",
    "            tokenized = tokenizer(batch['text'], padding=True, return_tensors='pt', max_length=512,truncation=True)['input_ids'].to(device)\n",
    "            loss_valid += model(tokenized, tokenized)[\"loss\"].item()\n",
    "        logging.info(f\"Final validation loss: {loss_valid}\")\n",
    "        save_checkpoint(model, optim, updates, model_filename)\n",
    "        # save trained model as artifact to wandb\n",
    "        wandb.log_artifact(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ca1fc-8797-407a-96a6-d2ad69df966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained model output (1 epoch)\n",
    "test_language_modeling(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
